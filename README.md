
# ğŸ“š LLM-Powered Research Assistant ğŸ¤–

An AI-powered research assistant that leverages Retrieval-Augmented Generation (RAG) to provide accurate responses to user queries by retrieving relevant documents and reasoning through complex questions.

## Features

- **Smart Query Routing**: Autonomously decides whether to answer directly from knowledge, retrieve additional context, or use specialized tools
- **RAG Pipeline**: Retrieves relevant documents to enhance responses with accurate, up-to-date information
- **Multi-step Reasoning**: Uses DSPy for structured reasoning to break down complex queries
- **Tool Integration**: Can utilize calculators, web search, and other external tools when needed
- **Evaluation Framework**: Measures response quality and relevance using DSPy's evaluation capabilities

## Architecture

```ascii
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Interface â”‚â”€â”€â”€â”€â–¶â”‚  Query Router   â”‚â”€â”€â”€â”€â–¶â”‚  RAG Pipeline   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚  â–²                     â”‚
                               â–¼  â”‚                     â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Tools (Calc,  â”‚     â”‚  Vector Store   â”‚
                        â”‚   Web Search)   â”‚     â”‚   (ChromaDB)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚  â–²                     â”‚
                               â–¼  â”‚                     â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   LLM Provider  â”‚â—€â”€â”€â”€â–¶â”‚  DSPy Modules   â”‚
                        â”‚ (OpenAI/Claude) â”‚     â”‚    & Metrics    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Setup & Installation

1. Clone the repository

```bash
git clone https://github.com/ako1983//LLM_research_assistant.git
cd /LLM_research_assistant
```

2. Install dependencies

```bash
pip install -r requirements.txt
```

3. Set up environment variables

```bash
export OPENAI_API_KEY="your-api-key"
export ANTHROPIC_API_KEY="your-api-key"  # If using Claude
```

4. Prepare your data

```bash
python src/vectorstore_builder.py
```

## Usage

```python
from src.agent import ResearchAssistant
from src.llm_providers import OpenAILLM
from src.rag_pipeline import RAGPipeline

# Initialize components
llm = OpenAILLM(model_name="gpt-3.5-turbo")
rag = RAGPipeline()
rag.initialize()
retriever = rag.get_retriever()

# Create and use the assistant
assistant = ResearchAssistant(llm_provider=llm, retriever=retriever)
response = assistant.process_query("How do I fix Wi-Fi connection issues?")
print(response["response"])
```

## Project Structure

```
llm-research-assistant/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original dataset files
â”‚   â”œâ”€â”€ processed/          # Cleaned CSV files
â”‚   â””â”€â”€ vector_stores/      # ChromaDB vector stores
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ query_classification_prompt_template.txt  # LLM prompts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py            # Main assistant logic
â”‚   â”œâ”€â”€ llm_providers.py    # LLM abstraction layer
â”‚   â”œâ”€â”€ rag_pipeline.py     # Document retrieval system
â”‚   â”œâ”€â”€ router.py           # Query routing logic
â”‚   â”œâ”€â”€ tools/              # External tool integrations
â”‚   â””â”€â”€ dspy_modules/       # DSPy components
â”œâ”€â”€ tests/                  # Test cases
â”œâ”€â”€ main.py                 # Entry point
â””â”€â”€ requirements.txt        # Dependencies
```

## Requirements

- Python 3.8+
- LangChain
- DSPy
- ChromaDB
- OpenAI or Anthropic API access

## Evaluation

The system uses DSPy's evaluation framework to assess:

- Answer correctness
- Context relevance
- Reasoning quality

## Acknowledgements

- Data sourced from [Bitext Customer Support Dataset](https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset)
- Built for an assessment
